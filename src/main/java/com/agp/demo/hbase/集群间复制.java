package com.agp.demo.hbase;

/**
 * HBase默认采用异步复制的方式同步数据，即客户端执行完put之后，
 * RegionServer的后台线程不断地推送HLog的Entry到Peer集群。
 *
 * HBase 1.x版本的复制功能，
 * 无法保证Region迁移前后的HLog的Entry按照严格一致的顺序推送到备集群，
 * 某些极端情况下可能造成主从集群数据不一致。
 * 为此，社区在HBase 2.x版本上实现了串行复制来解决这个问题。
 *
 * 默认的异步复制无法满足强一致性的跨机房热备需求。
 * 因为备份机房的数据肯定会落后主集群，一旦主集群异常，
 * 无法直接切换到备份集群，因此，社区提出并研发了同步复制。
 *
 * 业务方A对HBase集群的延迟和可用性要求非常高。
 * 现在又收到业务方B的需求，希望对表TableX跑数据分析任务
 * 如果直接去扫SSD在线集群，会极大影响集群的延迟和可用性，对业务方A来说不可接受
 *
 * 一个思路就是，用一批成本较低的HDD机器搭建一个离线的HBase集群，
 * 然后把表TableX的全量数据导入离线集群，
 * 再通过复制把增量数据实时地同步到离线集群，
 * 业务方B的分析任务直接跑在离线集群上。
 *
 * 步骤1：先确认表TableX的多个Column Family都已经将REPLICATION_SCOPE设为1。
 *
 * 步骤2：在SSD集群上添加一条DISABLED复制链路，
 * 提前把主集群正在写入的HLog堵在复制队列中[插图]，代码如下：
 *
 * 步骤3：对TableX做一个Snapshot，并用HBase内置的ExportSnapshot工具
 * 把Snapshot拷贝到离线集群上。
 * 注意，不要使用distcp拷贝snapshot，
 * 因为容易在某些情况下造成数据丢失。
 *
 * 步骤4：待Snapshot数据拷贝完成后，从Snapshot中恢复一个TableX表到离线集群。
 *
 * 步骤5：打开步骤1中添加的Peer：
 *
 * 步骤6：等待peer=100，所有堵住的HLog都被在线集群推送到离线集群，
 * 也就是两个集群的复制延迟等于0，就可以开始在离线集群上跑分析任务了。
 *
 *
 */
public class 集群间复制 {
}
