package com.agp.demo.es;

/**
 * 选主节点--》选集群元信息--》选主备shard--》分片recovery
 *
 *  分片分配成功后进入recovery流程。
 * 主分片的recovery不会等待其副分片分配成功才开始recovery。
 * 它们是独立的流程，只是副分片的recovery需要主分片恢复完毕才开始。
 *
 *  为什么需要recovery？
 *      对于主分片来说，可能有一些数据没来得及刷盘；
 *      对于副分片来说，
 *          一是没刷盘，
 *          二是主分片写完了，副分片还没来得及写，主副分片数据不一致。
 *  主分片recovery
 *      由于每次写操作都会记录事务日志（translog），事务日志中记录了哪种操作，
 *      以及相关的数据。因此将最后一次提交
 *      （Lucene 的一次提交就是一次 fsync 刷盘的过程）
 *      之后的 translog中进行重放，建立Lucene索引，如此完成主分片的recovery。
 *  副分片recovery
 *      副分片的恢复是比较复杂的，在ES的版本迭代中，副分片恢复策略有过不少调整。
 *
 *      副分片需要恢复成与主分片一致，
 *      同时，恢复期间允许新的索引操作。
 *      在目前的6.0版本中，恢复分成两阶段执行。
 *
 *      · phase1：在主分片所在节点，获取translog保留锁，从获取保留锁开始，
 *      会保留translog不受其刷盘清空的影响。
 *      然后调用Lucene接口把shard做快照，这是已经刷磁盘中的分片数据。
 *      把这些shard数据复制到副本节点。
 *      在phase1完毕前
 *      ，会向副分片节点发送告知对方启动engine，在phase2开始之前，副分片就可以正常处理写请求了。
 *
 *      · phase2：对translog做快照，这个快照里包含从phase1开始，到执行translog快照期间的新增索引。
 *      将这些translog发送到副分片所在节点进行重放。
 *
 *
 *
 *
 */
public class Recovery过程 {
}
